{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fe2eb6",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "2201210498 尹雯婧\n",
    "## 1. NLTK安装及用途\n",
    "### (1) NLTK简介\n",
    "NLTK，全称Natural Language Toolkit，自然语言处理工具包，是NLP研究领域常用的一个Python库，由宾夕法尼亚大学的Steven Bird和Edward Loper在Python的基础上开发的一个模块，至今已有超过十万行的代码。这是一个开源项目，包含数据集、Python模块、教程等。\n",
    "### (2) NLTK安装\n",
    "打开终端进行nltk安装\n",
    "```text\n",
    "pip install nltk\n",
    "```\n",
    "成功安装\n",
    "![image](images/1-1.png)\n",
    "\n",
    "打开终端依次输入以下内容来安装NLTK包\n",
    "```text\n",
    ">python\n",
    ">>>import nltk\n",
    ">>>nltk.download()\n",
    "```\n",
    "\n",
    "### (3) NLTK常见模块及用途\n",
    "![image](images/1-4.png)\n",
    "### (4) NLTK自带的语料库corpus\n",
    "nltk.corpus包提供了几类标注好的语料库：\n",
    "\n",
    "|语料库|       说明       |\n",
    "| :-------- | :----------- |\n",
    "|gutenberg| 一个有若干万部的小说语料库，多是古典作品|\n",
    "|webtext|收集的网络广告等内容|\n",
    "|nps_chat|有上万条聊天消息语料库，即时聊天消息为主|\n",
    "|brown|一个百万词级的英语语料库，按文体进行分类|\n",
    "|reuters|路透社语料库，上万篇新闻方档，约有1百万字，<br>分90个主题，并分为训练集和测试集两组|\n",
    "|inaugural|演讲语料库，几十个文本，都是总统演说|\n",
    "```python\n",
    "from nltk.corpus import brown\n",
    "print(brown.categories())   #输出brown语料库的类别\n",
    "print(len(brown.sents()))   #输出brown语料库的句子数量\n",
    "print(len(brown.words()))   #输出brown语料库的词数量\n",
    "\n",
    "'''\n",
    "结果为：\n",
    "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', \n",
    "'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', \n",
    "'science_fiction']\n",
    "57340\n",
    "1161192\n",
    "'''\n",
    "```\n",
    "## 2. NLTK词频统计\n",
    "NLTK 中的FreqDist() 类主要记录每个词出现的次数，根据统计数据生成表格或绘图。\n",
    "\n",
    "|方法|作用|\n",
    "|:---|:---|\n",
    "|B()|\t返回词典的长度|\n",
    "|plot(title,cumulative=False)|\t绘制频率分布图，若cumu为True，则是累积频率分布图|\n",
    "|tabulate()|\t生成频率分布的表格形式|\n",
    "|most_common()|\t返回出现次数最频繁的词与频度|\n",
    "|hapaxes()|\t返回只出现过一次的词|\n",
    "其实现如下：\n",
    "```python\n",
    "import nltk\n",
    "tokens = ['day', 'to', 'night', 'to', 'morning', 'keep', 'with',\n",
    "          'me', 'in', 'the', 'moment', 'I\\'d', 'let', 'you', 'had',\n",
    "          'I', 'known', 'it', 'why', 'don\\'t', 'you', 'say', 'so']\n",
    "# 统计词频\n",
    "freq = nltk.FreqDist(tokens)\n",
    "\n",
    "# 输出词和相应的频率\n",
    "for key, val in freq.items():\n",
    "    print(str(key) + ':' + str(val))\n",
    "\n",
    "# 可以把最常用的5个单词拿出来\n",
    "standard_freq = freq.most_common(5)\n",
    "print(standard_freq)\n",
    "```\n",
    "运行结果如下：\n",
    "```text\n",
    "day:1\n",
    "to:2\n",
    "night:1\n",
    "morning:1\n",
    "keep:1\n",
    "with:1\n",
    "me:1\n",
    "in:1\n",
    "the:1\n",
    "moment:1\n",
    "I'd:1\n",
    "let:1\n",
    "you:2\n",
    "had:1\n",
    "I:1\n",
    "known:1\n",
    "it:1\n",
    "why:1\n",
    "don't:1\n",
    "say:1\n",
    "so:1\n",
    "[('to', 2), ('you', 2), ('day', 1), ('night', 1), ('morning', 1)]\n",
    "```\n",
    "## 3. NLTK去除停用词 (stopwords)\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stwords = stopwords.words('english')\n",
    "print(stwords) # 打印英文停用词表\n",
    "```\n",
    "英文停用词表如下：<br>\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "<br>\n",
    "\n",
    "运行以下代码进行停用词清洗：\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokens = ['day', 'to', 'night', 'to', 'morning', 'keep', 'with',\n",
    "          'me', 'in', 'the', 'moment', 'I\\'d', 'let', 'you', 'had',\n",
    "          'I', 'known', 'it', 'why', 'don\\'t', 'you', 'say', 'so']\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "stwords = stopwords.words('english')\n",
    "for token in tokens:\n",
    "    if token in stwords:\n",
    "        clean_tokens.remove(token)\n",
    "\n",
    "print(clean_tokens)\n",
    "```\n",
    "运行结果如下：\n",
    "```text\n",
    "['day', 'night', 'morning', 'keep', 'moment', \"I'd\", 'let', 'I', 'known', 'say']\n",
    "```\n",
    "## 4. NLTK分词&分句 (tokenize)\n",
    "### (1) 对英文文本进行分句\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "mytext = \"Day to night to morning, keep with me in the moment. I'd let you had I known it. Why don't you say so?\"\n",
    "print(sent_tokenize(mytext))\n",
    "```\n",
    "分句结果如下：\n",
    "```text\n",
    "['Day to night to morning, keep with me in the moment.', \"I'd let you had I known it.\", \"Why don't you say so?\"]\n",
    "```\n",
    "### (2) 对英文文本进行分词\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "mytext = \"Day to night to morning, keep with me in the moment. I'd let you had I known it. Why don't you say so?\"\n",
    "print(word_tokenize(mytext))\n",
    "```\n",
    "分词结果如下：\n",
    "```text\n",
    "['Day', 'to', 'night', 'to', 'morning', ',', 'keep', 'with', 'me', 'in', 'the', 'moment', '.', \n",
    "'I', \"'d\", 'let', 'you', 'had', 'I', 'known', 'it', '.', 'Why', 'do', \"n't\", 'you', 'say', 'so', '?']\n",
    "```\n",
    "### (3) 对非英文文本进行分句\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "mytext = \"Del día a la noche a la mañana, mantente conmigo en el momento. \" \\\n",
    "         \"Te dejaría si lo supiera. ¿Por qué no lo dices?\"\n",
    "print(sent_tokenize(mytext, language='spanish'))  # 设定语言为西班牙语\n",
    "```\n",
    "分句结果如下：\n",
    "```text\n",
    "['Del día a la noche a la mañana, mantente conmigo en el momento.', 'Te dejaría si lo supiera.', '¿Por qué no lo dices?']\n",
    "```\n",
    "## 5. NLTK词干提取 (Stemming)\n",
    "词干提取最常用的办法就是 Porter 提取算法。NLTK中有一个PorterStemmer类，使用的就是Porter提取算法。\n",
    "\n",
    "举例来说，gets的词干是get。搜索引擎索引页面经常使用这种技术。效果就是，输入同一个单词的不同形式，返回的用于进行检索的值都是相同的。\n",
    "### (1) PorterStemmer\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('gets'))\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "get\n",
    "```\n",
    "### (2) LancasterStemmer\n",
    "```python\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('gets'))  #结果应该为：get\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "get\n",
    "```\n",
    "### (3) SnowballStemmer 提取非英语单词词干\n",
    "NLTK的SnowballStemmer类除了英语外，还适用于其他13种语言：\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', \n",
    "'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
    "```\n",
    "使用SnowballStemmer类中的stem()函数来提取非英语单词：\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "french_stemmer = SnowballStemmer('french')\n",
    "print(french_stemmer.stem(\"Jour après jour, nuit après matin, reste avec moi dans l'instant. \"\n",
    "                          \"Je te laisserais faire si je le savais. Pourquoi ne pas le dire ?\"))\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "jour après jour, nuit après matin, reste avec moi dans l'instant. \n",
    "je te laisserais faire si je le savais. pourquoi ne pas le dire ?\n",
    "```\n",
    "## 6. NLTK词形还原 (Lemmatization)\n",
    "### (1)\n",
    "词干提取经常会出现意想不到的问题，返回一个本来并不存在的单词。\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('increases'))\n",
    "```\n",
    "比如在提取increases这个词的词干的时候，提取结果为：\n",
    "```text\n",
    "increas\n",
    "```\n",
    "而词性还原的结果为一个真正的词。\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))  # 结果为：increase\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "increase\n",
    "```\n",
    "### (2)\n",
    "我们运行以下代码：\n",
    "```text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('getting'))\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "getting\n",
    "```\n",
    "并没有返回get的原因是，lemmatize()方法默认返回的词性是名词，如果我们想返回动词，可以通过以下方法进行指定。\n",
    "```text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('getting', pos=\"v\"))\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "get\n",
    "```\n",
    "lemmatize()方法其实是用到了WordNet所以才会准确返回真实存在的单词。  \n",
    "关于WordNet的使用会在后文进行介绍。\n",
    "### (3)\n",
    "lemmatize()方法返回的结果可以是动词(v)、名词(n)、形容词(a)、副词(r)。使用方法如下。\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('alleged', pos=\"v\"))  # 返回动词\n",
    "print(lemmatizer.lemmatize('friends', pos=\"n\"))  # 返回名词\n",
    "print(lemmatizer.lemmatize('previous', pos=\"a\"))  # 返回形容词\n",
    "print(lemmatizer.lemmatize('carelessly', pos=\"r\"))  # 返回副词\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "allege\n",
    "friend\n",
    "previous\n",
    "carelessly\n",
    "```\n",
    "词形还原方法将单词还原成原形形式，可以很有效地压缩文本量。\n",
    "## 7. NLTK词性标注(POS Tag)\n",
    "词性标注可以将一个句子中的单词标注为动词、名词、形容词以及副词等。\n",
    "```python\n",
    "import nltk\n",
    "text=nltk.word_tokenize('why don\\'t you say so?')\n",
    "print(text)\n",
    "print(nltk.pos_tag(text))\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "['why', 'do', \"n't\", 'you', 'say', 'so', '?']\n",
    "[('why', 'WRB'), ('do', 'VBP'), (\"n't\", 'RB'), ('you', 'PRP'), ('say', 'VBP'), ('so', 'RB'), ('?', '.')]\n",
    "```\n",
    "这其中出现的词性标注(Part of Speech)为：\n",
    "\n",
    "|标记(Tag)|含义(Meaning)|例子(Examples)|\n",
    "|----|-----|-----|\n",
    "|PRP|Personal Pronoun 人称代词|I, you, he|\n",
    "|RB|Adverb 副词|so, greatly|\n",
    "|VBP|Verb, non-3rd present 动词（非第三人称现在时）|do, say, go|\n",
    "|WRB|Wh-adverb Wh开头的副词|when, where, why|\n",
    "|.|Sentence-final punctuation 句尾标点|. ? !|\n",
    "\n",
    "pos_tag()方法厉害的地方在于可以根据上下文和句意判断具有多种词性的词在特定的句子或者短语当中分别是什么词性。\n",
    "```python\n",
    "import nltk\n",
    "text1=nltk.word_tokenize('cut with a saw')\n",
    "text2=nltk.word_tokenize('I saw him yesterday.')\n",
    "print(nltk.pos_tag(text1))\n",
    "print(nltk.pos_tag(text2))\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "[('cut', 'NN'), ('with', 'IN'), ('a', 'DT'), ('saw', 'NN')]\n",
    "[('I', 'PRP'), ('saw', 'VBD'), ('him', 'PRP'), ('yesterday', 'NN'), ('.', '.')]\n",
    "```\n",
    "这两句中saw的词性分别为名词和动词，可以看到给出的结果中，pos_tag()方法成功准确标注了两种不同的词性。\n",
    "## 8. NLTK中的WordNet\n",
    "WordNet是一个词与词之间存在各种关系的词典，不同于传统词典和同义词词典，WordNet有是以同义词集合(Synset)作为基本的构建单位来组织的，用户可以在同义词集合中找到一个合适的词去表达一个已知的概念。与传统词典类似，它也给出了指定词的定义和例句。\n",
    "\n",
    "WordNet把同义词集合以一定的关系类型联系起来，其中有同义关系(synonymy)、反义关系(antonymy)、上下位关系(hypernymy/hyponymy)、整体和部分关系(meronymy)和继承关系(entailment)等。\n",
    "\n",
    "在传统词典中的词条是多义词，会有多个解释。但在WordNet中，大多数的同义词集都有说明性的注释，但一个Synset不等于词典中的一个词条，因为一个Synset只包含一个注释。即“一个Synset等于一个词义”，以一条词义为一条数据。\n",
    "\n",
    "### (1) synsets结构\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"fast\")\n",
    "print(syn)\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "[Synset('fast.n.01'), Synset('fast.v.01'), Synset('fast.v.02'), Synset('fast.a.01'), Synset('fast.a.02'), \n",
    "Synset('fast.a.03'), Synset('fast.s.04'), Synset('fast.s.05'), Synset('debauched.s.01'), Synset('flying.s.02'), \n",
    "Synset('fast.s.08'), Synset('firm.s.10'), Synset('fast.s.10'), Synset('fast.r.01'), Synset('fast.r.02')]\n",
    "```\n",
    "### (2) 获取定义和例句\n",
    "注意也需要进行pos指定，否则默认为动词：\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"fast\")  #获取\"fast\"的同义词集\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "abstaining from food\n",
    "[]\n",
    "```\n",
    "fast的动词释义为“禁食”，这里在没有指定pos的情况下，默认返回了fast的动词词义。  \n",
    "将pos指定为形容词(a)之后：\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"fast\", pos='a')  #获取\"fast\"的同义词集\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "acting or moving or capable of acting or moving quickly\n",
    "['fast film', 'on the fast track in school', 'set a fast pace', 'a fast car']\n",
    "```\n",
    "### (3) 获取同义词\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('fancy', pos='v'):  # 同义词\n",
    "    for lemma in syn.lemmas():  # 词元\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "['visualize', 'visualise', 'envision', 'project', 'fancy', 'see', 'figure', 'picture', 'image', 'fancy', 'go_for', 'take_to']\n",
    "```\n",
    "### (4) 获取反义词\n",
    "WordNet中每一个单词是有其对应的反义词的，我们可以用“返回该词所有同义词的反义词”这种方法还获取该词的反义词。\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"fast\"):  # 同义词\n",
    "    for lemma in syn.lemmas():\n",
    "        if lemma.antonyms() and lemma.antonyms()[0].name() not in antonyms:   # 如果其同义词有反义词且不在antonyms里面\n",
    "            antonyms.append(lemma.antonyms()[0].name())  # 将这个同义词的反义词放入antonyms\n",
    "print(antonyms)\n",
    "```\n",
    "运行结果为：\n",
    "```text\n",
    "['slow']\n",
    "```\n",
    "\n",
    "#### 参考资料：\n",
    "[NLTK Corpora](http://www.nltk.org/nltk_data/)  \n",
    "[Natural Language Toolkit](https://www.nltk.org/)  \n",
    "[NLTK使用方法总结](https://blog.csdn.net/asialee_bird/article/details/85936784)  \n",
    "[WordNet用法及思路](https://zhuanlan.zhihu.com/p/26527203)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
